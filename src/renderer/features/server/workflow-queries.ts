import type { SupabaseClient } from '@supabase/supabase-js';
import type { Database } from '../../supa-client';

// ğŸ”¥ ì‚¬ìš©ìì˜ ì›Œí¬í”Œë¡œìš° ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (SSRìš©)
export const getUserWorkflows = async (
  client: SupabaseClient<Database>,
  params: {
    profile_id: string;
    status?: 'draft' | 'active' | 'archived' | 'shared';
    limit?: number;
    offset?: number;
  }
) => {
  const { profile_id, status, limit = 50, offset = 0 } = params;
  
  try {
    let query = client
      .from('workflows')
      .select(`
        *,
        profiles (
          profile_id,
          name,
          username,
          avatar
        )
      `)
      .eq('profile_id', profile_id)
      .order('updated_at', { ascending: false })
      .range(offset, offset + limit - 1);
      
    if (status) {
      query = query.eq('status', status);
    }
    
    const { data, error } = await query;
    if (error) throw error;
    return data || [];
  } catch (error) {
    console.error('Failed to fetch user workflows:', error);
    throw error;
  }
};

// ğŸ”¥ ì›Œí¬í”Œë¡œìš° ìƒì„¸ ì •ë³´ (ë…¸ë“œ+ì—£ì§€ í¬í•¨) (SSRìš©)
export const getWorkflowWithDetails = async (
  client: SupabaseClient<Database>,
  params: {
    workflow_id: number;
    profile_id: string;
  }
) => {
  const { workflow_id, profile_id } = params;
  
  try {
    // 1. ì›Œí¬í”Œë¡œìš° ê¸°ë³¸ ì •ë³´
    const { data: workflow, error: workflowError } = await client
      .from('workflows')
      .select(`
        *,
        profiles (
          profile_id,
          name,
          username,
          avatar
        )
      `)
      .eq('id', workflow_id)
      .eq('profile_id', profile_id)
      .single();
      
    if (workflowError) throw workflowError;
    if (!workflow) return null;
    
    // 2. ê´€ë ¨ ë…¸ë“œë“¤ (ì„œë²„ ì •ë³´ í¬í•¨)
    const { data: nodes, error: nodesError } = await client
      .from('workflow_nodes')
      .select(`
        *,
        mcp_servers:original_server_id (
          id,
          name,
          description,
          primary_url,
          github_info,
          metadata
        )
      `)
      .eq('workflow_id', workflow_id);
      
    if (nodesError) throw nodesError;
    
    // 3. ê´€ë ¨ ì—£ì§€ë“¤
    const { data: edges, error: edgesError } = await client
      .from('workflow_edges')
      .select('*')
      .eq('workflow_id', workflow_id);
      
    if (edgesError) throw edgesError;
    
    return {
      ...workflow,
      nodes: nodes || [],
      edges: edges || []
    };
  } catch (error) {
    console.error('Failed to fetch workflow details:', error);
    throw error;
  }
};

// ğŸ”¥ ì›Œí¬í”Œë¡œìš° ìƒì„± (SSRìš©)
export const createWorkflow = async (
  client: SupabaseClient<Database>,
  data: {
    profile_id: string;
    name: string;
    description?: string;
    flow_structure: any;
    status?: 'draft' | 'active' | 'archived' | 'shared';
    tags?: string[];
    is_public?: boolean;
    is_template?: boolean;
  }
) => {
  try {
    const { data: workflow, error } = await client
      .from('workflows')
      .insert({
        ...data,
        status: data.status || 'draft',
        is_public: data.is_public || false,
        is_template: data.is_template || false,
      })
      .select()
      .single();
      
    if (error) throw error;
    return workflow;
  } catch (error) {
    console.error('Failed to create workflow:', error);
    throw error;
  }
};

// ğŸ”¥ ì›Œí¬í”Œë¡œìš° ì—…ë°ì´íŠ¸ (SSRìš©)
export const updateWorkflow = async (
  client: SupabaseClient<Database>,
  params: {
    workflow_id: number;
    profile_id: string;
    data: Partial<{
      name: string;
      description: string;
      flow_structure: any;
      status: 'draft' | 'active' | 'archived' | 'shared';
      tags: string[];
      is_public: boolean;
      is_template: boolean;
    }>;
  }
) => {
  const { workflow_id, profile_id, data } = params;
  
  try {
    const { data: workflow, error } = await client
      .from('workflows')
      .update({
        ...data,
        updated_at: new Date().toISOString(),
      })
      .eq('id', workflow_id)
      .eq('profile_id', profile_id)
      .select()
      .single();
      
    if (error) throw error;
    return workflow;
  } catch (error) {
    console.error('Failed to update workflow:', error);
    throw error;
  }
};

// ğŸ”¥ ì›Œí¬í”Œë¡œìš° ë…¸ë“œ ì €ì¥ (SSRìš©)
export const saveWorkflowNodes = async (
  client: SupabaseClient<Database>,
  params: {
    workflow_id: number;
    nodes: Array<{
      node_id: string;
      node_type: string;
      position_x: number;
      position_y: number;
      node_config?: any;
      original_server_id?: number;
      user_mcp_usage_id?: number;
      client_id?: number;
    }>;
  }
) => {
  const { workflow_id, nodes } = params;
  
  try {
    // 1. ê¸°ì¡´ ë…¸ë“œë“¤ ì‚­ì œ
    await client
      .from('workflow_nodes')
      .delete()
      .eq('workflow_id', workflow_id);
    
    // 2. ìƒˆ ë…¸ë“œë“¤ ì‚½ì…
    if (nodes.length > 0) {
      const { data, error } = await client
        .from('workflow_nodes')
        .insert(
          nodes.map(node => ({
            workflow_id,
            ...node,
          }))
        )
        .select();
        
      if (error) throw error;
      return data;
    }
    
    return [];
  } catch (error) {
    console.error('Failed to save workflow nodes:', error);
    throw error;
  }
};

// ğŸ”¥ ì›Œí¬í”Œë¡œìš° ì—£ì§€ ì €ì¥ (SSRìš©)
export const saveWorkflowEdges = async (
  client: SupabaseClient<Database>,
  params: {
    workflow_id: number;
    edges: Array<{
      edge_id: string;
      source_node_id: string;
      target_node_id: string;
      source_handle?: string;
      target_handle?: string;
      edge_config?: any;
    }>;
  }
) => {
  const { workflow_id, edges } = params;
  
  try {
    // 1. ê¸°ì¡´ ì—£ì§€ë“¤ ì‚­ì œ
    await client
      .from('workflow_edges')
      .delete()
      .eq('workflow_id', workflow_id);
    
    // 2. ìƒˆ ì—£ì§€ë“¤ ì‚½ì…
    if (edges.length > 0) {
      const { data, error } = await client
        .from('workflow_edges')
        .insert(
          edges.map(edge => ({
            workflow_id,
            ...edge,
          }))
        )
        .select();
        
      if (error) throw error;
      return data;
    }
    
    return [];
  } catch (error) {
    console.error('Failed to save workflow edges:', error);
    throw error;
  }
};

// ğŸ”¥ ì›Œí¬í”Œë¡œìš° ì‚­ì œ (SSRìš©)
export const deleteWorkflow = async (
  client: SupabaseClient<Database>,
  params: {
    workflow_id: number;
    profile_id: string;
  }
) => {
  const { workflow_id, profile_id } = params;
  
  try {
    // 1. ê´€ë ¨ ë°ì´í„°ë“¤ ë¨¼ì € ì‚­ì œ (ìˆœì„œ ì¤‘ìš”)
    await client.from('workflow_nodes').delete().eq('workflow_id', workflow_id);
    await client.from('workflow_edges').delete().eq('workflow_id', workflow_id);
    await client.from('workflow_executions').delete().eq('workflow_id', workflow_id);
    await client.from('workflow_shares').delete().eq('workflow_id', workflow_id);
    
    // 2. ì›Œí¬í”Œë¡œìš° ì‚­ì œ
    const { data, error } = await client
      .from('workflows')
      .delete()
      .eq('id', workflow_id)
      .eq('profile_id', profile_id)
      .select()
      .single();
      
    if (error) throw error;
    return data;
  } catch (error) {
    console.error('Failed to delete workflow:', error);
    throw error;
  }
};

// ğŸ”¥ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ê¸°ë¡ ìƒì„± (SSRìš©)


// ğŸ”¥ ì‚¬ìš©ì ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ê¸°ë¡ (SSRìš©)
export const getUserWorkflowExecutions = async (
  client: SupabaseClient<Database>,
  params: {
    profile_id: string;
    workflow_id?: number;
    limit?: number;
    offset?: number;
  }
) => {
  const { profile_id, workflow_id, limit = 50, offset = 0 } = params;
  
  try {
    let query = client
      .from('workflow_executions')
      .select(`
        *,
        workflows:workflow_id (
          id,
          name
        )
      `)
      .eq('profile_id', profile_id)
      .order('started_at', { ascending: false })
      .range(offset, offset + limit - 1);
      
    if (workflow_id) {
      query = query.eq('workflow_id', workflow_id);
    }
    
    const { data, error } = await query;
    if (error) throw error;
    return data || [];
  } catch (error) {
    console.error('Failed to fetch workflow executions:', error);
    throw error;
  }
};

// ğŸ”¥ ê³µê°œ ì›Œí¬í”Œë¡œìš°/í…œí”Œë¦¿ ì¡°íšŒ (SSRìš©)
export const getPublicWorkflows = async (
  client: SupabaseClient<Database>,
  params: {
    is_template?: boolean;
    limit?: number;
    offset?: number;
  } = {}
) => {
  const { is_template, limit = 20, offset = 0 } = params;
  
  try {
    let query = client
      .from('workflows')
      .select(`
        id,
        name,
        description,
        tags,
        is_template,
        execution_count,
        created_at
      `)
      .eq('is_public', true)
      .order('execution_count', { ascending: false })
      .order('created_at', { ascending: false })
      .range(offset, offset + limit - 1);
      
    if (is_template !== undefined) {
      query = query.eq('is_template', is_template);
    }
    
    const { data, error } = await query;
    if (error) throw error;
    return data || [];
  } catch (error) {
    console.error('Failed to fetch public workflows:', error);
    throw error;
  }
};

// ğŸ”¥ ê³µìœ  í† í°ìœ¼ë¡œ ì›Œí¬í”Œë¡œìš° ê°€ì ¸ì˜¤ê¸° (SSRìš©)
export const getWorkflowByShareToken = async (
  client: SupabaseClient<Database>,
  params: {
    share_token: string;
  }
) => {
  const { share_token } = params;
  
  try {
    // 1. ê³µìœ  ì •ë³´ ë¨¼ì € ì¡°íšŒ (ê¶Œí•œ í™•ì¸)
    const { data: shareData, error: shareError } = await client
      .from('workflow_shares')
      .select(`
        *,
        workflows (
          id,
          name,
          description,
          flow_structure,
          created_at,
          updated_at,
          profiles (
            profile_id,
            name,
            username,
            avatar
          )
        )
      `)
      .eq('share_token', share_token)
      .eq('is_active', true)
      .single();
      
    if (shareError) throw shareError;
    if (!shareData) {
      throw new Error('ì›Œí¬í”Œë¡œìš° ê³µìœ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
    }
    
    // 2. ë§Œë£Œ ì‹œê°„ í™•ì¸
    if (shareData.expires_at && new Date(shareData.expires_at) < new Date()) {
      throw new Error('ê³µìœ  ë§í¬ê°€ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤.');
    }
    
    // 3. ë·° ê¶Œí•œ í™•ì¸
    if (!shareData.can_view) {
      throw new Error('ì´ ì›Œí¬í”Œë¡œìš°ë¥¼ ë³¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤.');
    }
    
    const workflow = shareData.workflows;
    if (!workflow) {
      throw new Error('ì›Œí¬í”Œë¡œìš°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
    }
    
    // 4. ê´€ë ¨ ë…¸ë“œë“¤ (ì„œë²„ ì •ë³´ í¬í•¨)
    const { data: nodes, error: nodesError } = await client
      .from('workflow_nodes')
      .select(`
        *,
        mcp_servers:original_server_id (
          id,
          name,
          description,
          primary_url,
          github_info,
          metadata
        )
      `)
      .eq('workflow_id', workflow.id);
      
    if (nodesError) throw nodesError;
    
    // 5. ê´€ë ¨ ì—£ì§€ë“¤
    const { data: edges, error: edgesError } = await client
      .from('workflow_edges')
      .select('*')
      .eq('workflow_id', workflow.id);
      
    if (edgesError) throw edgesError;
    
    // 6. ë‹¤ìš´ë¡œë“œ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸ (ì„ íƒì )
    await client
      .from('workflow_shares')
      .update({ 
        download_count: (shareData.download_count || 0) + 1 
      })
      .eq('id', shareData.id);
    
    return {
      workflow: {
        ...workflow,
        nodes: nodes || [],
        edges: edges || []
      },
      shareInfo: {
        share_token: shareData.share_token,
        share_title: shareData.share_title,
        share_description: shareData.share_description,
        can_view: shareData.can_view,
        can_copy: shareData.can_copy,
        can_edit: shareData.can_edit,
        download_count: shareData.download_count,
        created_at: shareData.created_at,
        expires_at: shareData.expires_at,
      }
    };
  } catch (error) {
    console.error('Failed to fetch workflow by share token:', error);
    throw error;
  }
};

// ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ê¸°ë¡ ì €ì¥ (ìƒˆë¡œìš´ ì‹¤í–‰ ì‹œì‘ ì‹œ)
export async function saveWorkflowExecution(
  client: SupabaseClient,
  payload: {
    workflow_id: number;
    user_id: string;
    execution_id: string;
    status?: 'running' | 'completed' | 'failed' | 'cancelled';
    result_data?: any;
    error_message?: string;
    nodes_executed?: number;
    nodes_failed?: number;
  }
) {
  console.log('ğŸ”¥ [saveWorkflowExecution] ì‹¤í–‰ ê¸°ë¡ ì €ì¥:', payload);
  
  const { data, error } = await client
    .from('workflow_executions')
    .insert({
      workflow_id: payload.workflow_id,
      user_id: payload.user_id,
      execution_id: payload.execution_id,
      status: payload.status || 'running',
      result_data: payload.result_data || null,
      error_message: payload.error_message || null,
      nodes_executed: payload.nodes_executed || 0,
      nodes_failed: payload.nodes_failed || 0,
    })
    .select()
    .single();

  if (error) {
    console.error('âŒ [saveWorkflowExecution] ì €ì¥ ì‹¤íŒ¨:', error);
    throw error;
  }

  console.log('âœ… [saveWorkflowExecution] ì €ì¥ ì„±ê³µ:', data);
  return data;
}

// ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ê¸°ë¡ ì—…ë°ì´íŠ¸ (ì™„ë£Œ/ì‹¤íŒ¨ ì‹œ)
export async function updateWorkflowExecution(
  client: SupabaseClient,
  executionId: string,
  updates: {
    status: 'completed' | 'failed' | 'cancelled';
    result_data?: any;
    error_message?: string;
    duration_ms?: number;
    nodes_executed?: number;
    nodes_failed?: number;
  }
) {
  console.log('ğŸ”„ [updateWorkflowExecution] ì‹¤í–‰ ê¸°ë¡ ì—…ë°ì´íŠ¸:', { executionId, updates });
  
  const { data, error } = await client
    .from('workflow_executions')
    .update({
      ...updates,
      completed_at: new Date().toISOString(),
    })
    .eq('execution_id', executionId)
    .select()
    .single();

  if (error) {
    console.error('âŒ [updateWorkflowExecution] ì—…ë°ì´íŠ¸ ì‹¤íŒ¨:', error);
    throw error;
  }

  console.log('âœ… [updateWorkflowExecution] ì—…ë°ì´íŠ¸ ì„±ê³µ:', data);
  return data;
}

// ì›Œí¬í”Œë¡œìš° ê³µìœ /í…œí”Œë¦¿ ì €ì¥
export async function createWorkflowShare(
  client: SupabaseClient,
  payload: {
    workflow_id: number;
    shared_by_user_id: string;
    share_type?: 'template' | 'public' | 'link';
    share_title?: string;
    share_description?: string;
    share_token?: string;
    can_view?: boolean;
    can_copy?: boolean;
    can_edit?: boolean;
    expires_at?: string;
  }
) {
  console.log('ğŸ“¤ [createWorkflowShare] ê³µìœ /í…œí”Œë¦¿ ìƒì„±:', payload);
  
  const { data, error } = await client
    .from('workflow_shares')
    .insert({
      workflow_id: payload.workflow_id,
      shared_by_user_id: payload.shared_by_user_id,
      share_type: payload.share_type || 'link',
      share_title: payload.share_title,
      share_description: payload.share_description,
      share_token: payload.share_token || `share_${Date.now()}`,
      can_view: payload.can_view ?? true,
      can_copy: payload.can_copy ?? true,
      can_edit: payload.can_edit ?? false,
      expires_at: payload.expires_at,
    })
    .select()
    .single();

  if (error) {
    console.error('âŒ [createWorkflowShare] ì €ì¥ ì‹¤íŒ¨:', error);
    throw error;
  }

  console.log('âœ… [createWorkflowShare] ì €ì¥ ì„±ê³µ:', data);
  return data;
} 